---
title: "STARS_SSN_V4"
author: "Luke Vawter"
date: "2/23/2022"
output: html_document
---

This is a modification of STARS_SSN_V2(which ran to completion but gave bad output).
This is the first version to run to completion and give full outputs, including predictions.This should be the template for ssn runs moving forward.

---
title: "STARS_SSN"
output: html_document
---

there is sample code for running an SSN prediction model at:

https://www.fs.fed.us/rm/boise/AWAE/projects/NorWeST/downloads/NorWeST%20SSN%20example%20code.R

When sample code is referenced here, this is what is being referred to



the SSN library is imported. A path for our the SSN data is set. This data is an output from Andrea Nagel's run of STARS.
```{r}
library(SSN)
library(dplyr)
library(lubridate)

 data_path ='data/ICPRB4/lsn.ssn'
 
```

functions are created for standardizing variables and predictions, that will be used later. These have been pulled directly from the sample code.
```{r}
# Function to standardize variables
stand <- function(x) { (x-mean(x))/(2*sd(x))}

# Function to standardize a prediction dataset based on the fitting dataset 
stdpreds <- function(newset,originalset) {
	xnames <- colnames(newset)
	sx <- matrix(rep(NA,ncol(newset)*nrow(newset)),nrow=nrow(newset))
	for(i in 1:ncol(newset)) {
		var <- with(originalset,get(xnames[i]))
		sx[,i] <- (newset[,i]-mean(var))/(2*sd(var))
		}
	colnames(sx) <- colnames(newset)
	return(sx)
}
```

import the from our data to an ssn object
```{r}

test.ssn <- importSSN(data_path, predpts = "preds", o.write = T)
```

from our ssn object we grab a datafram,remove the underscores from the variables, and push the dataframe back intot he ssn object. We also convert DATE value to julian. 
AREAKM2 and TOTDAKM2 removed

change all -9999 to NA or no data
```{r}
test_temp.df<- getSSNdata.frame(test.ssn) 

###was used to convert non values to na. removed in this version because it was causing an error

# test_temp.df <- test_temp.df %>%
#   mutate(TWAT = case_when(FLOW == -9999.00~ NA_real_,FLOW == -9999.00000~ NA_real_, TRUE~TWAT)) %>%
#   mutate(TAIR = case_when(FLOW == -9999.00~ NA_real_, TRUE~TAIR)) %>%
#   mutate(FLOW = case_when(FLOW == -9999.00~ NA_real_, TRUE~FLOW)) 

# test_temp.df <- na.omit(test_temp.df)

# test_temp.df<-rename(test_temp.df, 'DATE' = 'DATE_' ) 
# test_temp.df<-rename(test_temp.df, 'MONTH' = 'MONTH_' ) 
# test_temp.df<-rename(test_temp.df, 'YEAR' = 'YEAR_' )

test_temp.df<- select(test_temp.df, -AREAKM2, -TOTDAKM2)

test_temp.df <- test_temp.df %>%
  mutate(DATE_ = as.Date(test_temp.df$DATE_, "%Y-%m-%d")) %>%
  mutate(DATE_ =  yday(test_temp.df$DATE_))

test.ssn <- putSSNdata.frame(test_temp.df,test.ssn,"Obs")
test_temp.df<- getSSNdata.frame(test.ssn) 
```

Save the ssn dataframe to our data folder for reference
```{r}
write.csv(test.ssn,"data/ICPRB4/test_ssn.csv")
```

createDistMat function creates a distance matrix. Memory limits for R needed to be increased to run. I set it to 200000 MB, and that was sufficient for our data. Predpts are named preds in our lsn.ssn file and so are given the value 'preds'.
```{r}
#had to check memory and then adjust memory limit
memory.size()
memory.limit()
memory.limit(size=200000)

#dist_mat_test = 
createDistMat(test.ssn, o.write=T, predpts = 'preds', amongpreds=T)
# ssn, predpts = NULL, o.write = FALSE, amongpreds = FALSE)
```

create Torgegram
```{r}
# Create raw torgegram
test_torg <- Torgegram(test.ssn,"TWAT",nlag=20)

```

plot Torgegram for connected and unconnected(should be dipslayed in kilometers)and save both as jpegs
```{r}
jpeg("data/ICPRB4/torgegram_flow_connected.jpg")
plot.Torgegram(test_torg, sp.relationship = c("fc"), xlab = "Stream Distance (meter)")
dev.off()

jpeg("data/ICPRB4/torgegram_flow_unconnected.jpg")
plot.Torgegram(test_torg, sp.relationship = c("fu"), xlab = "Stream Distance (meter)")
dev.off()
```

Extract dataframe and fit basic aspatial model with un-standardized predictors
note: AREAKM2 + TOTDAKM2 removed
```{r}
test.df<- getSSNdata.frame(test.ssn)
# test.df<- select(test.df, -AREAKM2, -TOTDAKM2)
# test.lm <- lm(TWAT ~ TAIR + PRECIP + SOLRAD + FLOW + ELEV + SLOPE + PURB16 + PFOR16 + PAG16 + PWSTOR16 + PCAN14 + PIMP14 + PEWET + PFWET + PRWET + PHYDA + AREAKM2 + TOTDAKM2, data=test.df)
test.lm <- lm(TWAT ~ TAIR + PRECIP + SOLRAD + FLOW + ELEV + SLOPE + PURB16 + PFOR16 + PAG16 + PWSTOR16 + PCAN14 + PIMP14 + PEWET + PFWET + PRWET + PHYDA, data=test.df)
summary(test.lm)
```

# Standardize continuous covariates and add factor for year
The following steps are taken from sample code and adapted to fit our data indices/covariates
note: AREAKM2 + TOTDAKM2 removed
```{r}
# Standardize continuous covariates and add factor for year
continuous <- test.df[,c(13:28)] #[,c(14:29)]
cont.s <- apply(continuous,2,stand)
colnames(cont.s) <- c("TAIR","PRECIP","SOLRAD","FLOW","ELEV","SLOPE","PURB16","PFOR16","PAG16", "PWSTOR16", "PCAN14", "PIMP14", "PEWET", "PFWET", "PRWET", "PHYDA") #"AREAKM2","TOTDAKM2")
test.df.s <- data.frame(test.df,cont.s)
# test.df.s$yearf <- factor(test.df.s$YEAR)

#convert DATE to julian
# test.df.s <- test.df.s %>%
#   mutate(DATE_ = as.Date(test.df.s$DATE_, "%Y-%m-%d")) %>%
#   mutate(DATE_ =  yday(test.df.s$DATE_))

test.df.s$datef <- factor(test.df.s$DATE_)

# print(typeof(test.df.s$datef))
test_master <- putSSNdata.frame(test.df.s,test.ssn,"Obs")
```

esttable is saved
note: AREAKM2 + TOTDAKM2 removed

flows, twat and tair were being dropped by summary function because they included NA values 
```{r}
library(tibble)
testAe <- summary(test.lm)$coefficients

# #create row to replace missing FLOW row
# insert_flow <-data.frame(row.names = "FLOW", "Estimate" = NA_real_,"Std. Error" = NA_real_, "t value" = NA_real_,    "Pr(>|t|)" = NA_real_)
# #change colnames so ththat insert_rows can be bound to rest of data
# colnames(insert_flow) <- colnames(testAe)
# 
# #bind rows before and after with FLOW replacement row
# testAe <-rbind( testAe[0:4,],insert_flow[1,],  testAe[5:16,]) 



backtrans <- testAe[-c(1),1:2]/(2*sapply(continuous,sd))

esttable <- cbind(rbind(testAe[1,1:2],backtrans[1:15,],testAe[10,1:2]),testAe)#[1:17,],testAe[10,1:2]),testAe)

#fixes row names by overwriting them
rownames(esttable) <- rownames(testAe)

write.csv(esttable,"data/ICPRB4/test_aspatialestimates.csv")

# # Aspatial performance
predictA <- predict(test.lm)
sqrt(mean((predictA-test.df.s$TWAT)^2))
# # print(predictA)
# # 1.885
```

# Examine correlations
Generate a correlation plot and save it to data folder
```{r}
# Examine correlations
library(ellipse)
library(corrplot)

#color for legend
COL1(sequential = c("Oranges", "Purples", "Reds", "Blues", "Greens",
                    "Greys", "OrRd", "YlOrRd", "YlOrBr", "YlGn"), n = 200)

cor1 = cor(cont.s)
jpeg("data/ICPRB4/corrplot.jpg")
corrplot(cor1, type='lower',col=COL1())
# plotcorr(cor(cont.s), col=COL1,type="lower")
dev.off()

#NOTE: call the legend for this output
```

# Standardize preds based on obs and add factor for year- pred
Code is adapted from sample code to fit our data indices/covariates and output is saved 
note: AREAKM2 + TOTDAKM2 removed
```{r}
testpreddf <- getSSNdata.frame(test.ssn, "preds")

# testgetssn <- getSSNdata.frame(test.ssn, "preds")
# testgetssn<-rename(test_temp.df, 'AREAKM2' = 'AREA_KM2' ) 
# contpred <- testpreddf[,c(10:18,20:21)]
contpred <- testpreddf[,c(13:28)]#[,c(14:29)]
contpred.s <- stdpreds(contpred,continuous)

 colnames(contpred.s) <-
c("TAIR", "PRECIP", "SOLRAD", "FLOW", "ELEV", "SLOPE", "PURB16", "PFOR16", "PAG16", "PWSTOR16", "PCAN14", "PIMP14", "PEWET", "PWFET", "PRWET", "PHYDA")#, "AREAKM2","TOTDAKM2")
testpreddf.s <- data.frame(testpreddf,contpred.s)
testpreddf.s$datef <- factor(testpreddf.s$DATE_)
test_master <- putSSNdata.frame(testpreddf.s,test_master,"preds")

# write.csv(test_master,"data/ICPRB4/test_master.csv")

testmaster <- getSSNdata.frame(test_master, "preds") 
write.csv(testmaster,"data/ICPRB4/testmaster.csv")
```

### -Model # 1- ###

"The glmssn function fits a spatial linear model (Equation 3) to a SpatialStreamNetwork
object with a covariance structure"

The glmssn function can take hours to run. 

"This limits the size of data sets that can be fit by the glmssn
function, and for most computing systems today we suggest sample sizes of less than 1000 observations."
https://www.fs.fed.us/rm/boise/AWAE/projects/SSN_STARS/downloads/SSN/SSNvignette2014.pdf
page21

Also, we track glmssn run time(elapsed variable), as recommended by sample code

note: AREAKM2 + TOTDAKM2 removed
```{r}
# starttime <- Sys.time() # not necessary; just for timing longer runs
# test_1 <- glmssn(formula = TWAT ~ TAIR + PRECIP + SOLRAD + FLOW + ELEV + SLOPE + PURB16 + PFOR16 + PAG16 + PWSTOR16 + PCAN14 + PIMP14 + PEWET + PFWET + PRWET + PHYDA + AREAKM2 + TOTDAKM2, ssn.object = test_master, EstMeth= "ML", family="gaussian", CorModels = c("locID","yearf","Exponential.tailup","Exponential.taildown","Exponential.Euclid"), addfunccol = "afvArea")



starttime <- Sys.time() # not necessary; just for timing longer runs
test_1 <- glmssn(formula = TWAT ~ TAIR + PRECIP + SOLRAD + FLOW + ELEV + SLOPE + PURB16 + PFOR16 + PAG16 + PWSTOR16 + PCAN14 + PIMP14 + PEWET + PFWET + PRWET + PHYDA, ssn.object = test_master, EstMeth= "ML", family="gaussian", CorModels = c("locID","datef","Exponential.tailup","Exponential.taildown","Exponential.Euclid"), addfunccol = "afvArea")


elapsed <- Sys.time()-starttime # See above note
# elapsed

# help(glmssn)
```

# Extract predictions/ Leave-one-out cross validation predictions
```{r}
# Extract predictions/ Leave-one-out cross validation predictions
test_1r <- residuals(test_1, cross.validation=T)
test_1r.df <- getSSNdata.frame(test_1r)
```

generate a Torgegram of fitted model and save
```{r}
# Torgegram of fitted model residuals
test_1t <- Torgegram(test_1r,"_resid.crossv_",nlag=20)
jpeg("data/ICPRB4/model_torgegram_full.jpg")
plot(test_1t, main= "Model1 Residuals Torgegram") 
dev.off()

jpeg("data/ICPRB4/model_1_flow_connected.jpg")
plot.Torgegram(test_1t, sp.relationship = c("fc"), xlab = "Stream Distance (meter)")
dev.off()

jpeg("data/ICPRB4/model_1_flow_unconnected.jpg")
plot.Torgegram(test_1t, sp.relationship = c("fu"), xlab = "Stream Distance (meter)")
dev.off()
```

```{r}
#Root mean squared error (RMSE) of cross-validated predictions
sqrt(mean((test_1r.df[,"_CrossValPred_"]-test_1r.df$obsval)^2))
# 0.969


#RMSE of fixed effects only
sqrt(mean((test_1r.df[,"_fit_"]-test_1r.df$obsval)^2))
# 1.935


# Null RMSE
sqrt(mean((test_1r.df$obsval-mean(test_1r.df$obsval))^2))
# 2.996

#Pseudo-r2 of cross-validated predictions. 
cor(test_1r.df$obsval,test_1r.df[,"_CrossValPred_"])^2
# 0.895
```

Get parameter estimates, back-transform, and save estimates as model_1_estimates

note: AREAKM2 + TOTDAKM2 removed
```{r}
test_1e <- summary(test_1)$fixed.effects.estimates
# backtrans <- test_1e[-c(1,10),2:3]/(2*sapply(continuous[,c(1:7,9:11)],sd))
m1_backtrans <- test_1e[-c(0,10),2:3]/(2*sapply(continuous[,c(1:7,9:11)],sd))#continuous[,c

# esttable <- cbind(test_1e[,1],rbind(test_1e[1,2:3],m1_backtrans[1:17,],test_1e[10,2:3]),test_1e[,2:5])

esttable <- cbind(test_1e[,1],rbind(test_1e[1,2:3],m1_backtrans[1:15,],test_1e[10,2:3]),test_1e[,2:5])

write.csv(esttable,"data/ICPRB4/model_1_estimates.csv")

```


### Predictions ###

sample code 

recommends 

Data can be split using the splitPredictions if the data has multiple prediction point files in .ssn folder. We only have preds, and a managable dataset size. Below code has been commmented out.
```{r}
# library(SSN)
# test_1$ssn.object <- splitPredictions(test_1$ssn.object, "preds-1",chu
# nksof=10000)
# # splitPredictions
# print(test_1$ssn.object)

```

Running predictions generates an error:

Error in `[.data.frame`(datap, , mod.names[i]) : 
  undefined columns selected
  
A quick internet search shows that this error occurs when you attempt to subset data but leave out a comma, which doesn't apply to anything that is being explicitely done directly by our code here. Below code has been commented out.
```{r}
# library(SSN)
# print.summary.glmssn
# 
# 
# #make a prediction
# test1p1 <- predict.glmssn(test_1,predpointsID = "preds")
test1p1 <- predict(test_1,predpointsID = "preds")

# # Extract prediction data frames
pred1df <- getSSNdata.frame(test1p1,"preds")


```

Porecess of reassembling data will look something like this. This has been commented out but left in for reference.
```{r}
# Reassemble the pieces into one batch.
#not required since wasn't split

# allpreds <- rbind(pred1df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")],pred2df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")],pred3df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")],pred4df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")],pred5df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")],pred6df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")],pred7df[,c("OBSPREDID","STREAM_AUG","STREAM_AUG.predSE")])
# colnames(allpreds) <- c("OBSPREDID","predtemp","predtempse")

```

Export prediction dataset as a csv (good for general use) and dbf (good for GIS) Commented out as there are currently no predictions to save.
```{r}
library(foreign)

write.csv(pred1df,"data/ICPRB4/model_1_predictions.csv",row.names=F)
write.dbf(pred1df,"data/ICPRB4/model_1_predictions.dbf")

```

Compare predicted and observed at fitting sites, and export. Commented out, no predictions
```{r}

# predobs <- data.frame(test_1r.df$OBSPREDID,test_1r.df[,"_CrossValPred_"],test_1r.df$obsval)
# colnames(predobs) <- c("obspredid","predicted","observed")
# write.csv(predobs,"data/ICPRB4/model_1_prediction_observations.csv",row.names=F)

# test_obs <- test_1r.df$OBSPREDID
```
